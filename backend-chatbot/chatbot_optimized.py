"""
Version s√©curis√©e du chatbot avec gestion des variables d'environnement
"""

import json
import os
import pickle
import numpy as np
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import unicodedata
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Import conditionnel du correcteur orthographique
try:
    from spellchecker import preprocess_question
except ImportError:
    def preprocess_question(question: str) -> str:
        """Fallback si le correcteur orthographique n'est pas disponible"""
        return question.strip()

def remove_accents(text: str) -> str:
    """Supprime les accents d'un texte pour √©viter les probl√®mes d'encodage"""
    if not text:
        return text
    # Normaliser et supprimer les accents
    nfkd_form = unicodedata.normalize('NFD', text)
    ascii_text = ''.join([c for c in nfkd_form if not unicodedata.combining(c)])
    return ascii_text

def safe_encode_text(text: str) -> str:
    """Encode un texte de mani√®re s√ªre pour l'API"""
    if not text:
        return text
    
    # Remplacer les caract√®res probl√©matiques
    replacements = {
        '√©': 'e', '√®': 'e', '√™': 'e', '√´': 'e',
        '√†': 'a', '√¢': 'a', '√§': 'a',
        '√Æ': 'i', '√Ø': 'i',
        '√¥': 'o', '√∂': 'o',
        '√π': 'u', '√ª': 'u', '√º': 'u',
        '√ß': 'c',
        '√±': 'n'
    }
    
    result = text
    for accent, replacement in replacements.items():
        result = result.replace(accent, replacement)
        result = result.replace(accent.upper(), replacement.upper())
    
    # Supprimer tout caract√®re non-ASCII restant
    result = result.encode('ascii', errors='ignore').decode('ascii')
    return result

class ChatbotRHOptimise:
    """
    Chatbot RH optimis√© avec gestion s√©curis√©e des variables d'environnement
    """
    
    def __init__(self, data_path: str = None):
        # Configuration depuis les variables d'environnement
        self.data_path = data_path or os.getenv("DATA_PATH", "data/Nestle-HR-FAQ.json")
        self.seuil_confiance = float(os.getenv("CONFIDENCE_THRESHOLD", "1.0"))
        
        # Configuration du vectorizer depuis l'environnement
        self.vectorizer = TfidfVectorizer(
            max_features=int(os.getenv("TFIDF_MAX_FEATURES", "5000")),
            ngram_range=(1, int(os.getenv("TFIDF_NGRAM_RANGE", "2"))),
            stop_words=None,
            lowercase=True,
            min_df=int(os.getenv("TFIDF_MIN_DF", "1")),
            max_df=float(os.getenv("TFIDF_MAX_DF", "0.95"))
        )
        
        # Configuration du classifier depuis l'environnement
        self.classifier = Pipeline([
            ('tfidf', TfidfVectorizer(
                max_features=int(os.getenv("CLASSIFIER_MAX_FEATURES", "3000")), 
                ngram_range=(1, int(os.getenv("CLASSIFIER_NGRAM_RANGE", "2")))
            )),
            ('nb', MultinomialNB(alpha=float(os.getenv("NAIVE_BAYES_ALPHA", "0.1"))))
        ])
        
        # Donn√©es
        self.questions = []
        self.reponses = []
        self.themes = []
        self.tfidf_matrix = None
        self.theme_data = {}
        
        # Statistiques
        self.stats = {
            "questions_posees": 0,
            "reponses_trouvees": 0,
            "questions_incomprises": 0
        }
        
        # Configuration OpenAI s√©curis√©e
        self._initialiser_client_openai()
    
    def _initialiser_client_openai(self):
        """Initialise le client OpenAI avec les variables d'environnement"""
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            base_url = os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1")
            
            if not api_key:
                print("‚ö†Ô∏è Cl√© API OpenAI manquante dans le fichier .env")
                self.client = None
                return
            
            self.client = OpenAI(
                base_url=base_url,
                api_key=api_key
            )
            print("‚úÖ Client OpenAI initialis√© avec succ√®s")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'initialisation du client OpenAI: {e}")
            self.client = None
    
    def nettoyer_texte(self, texte: str) -> str:
        """Nettoie, corrige et normalise le texte"""
        if not texte:
            return ""
        try:
            texte_corrige = preprocess_question(texte)
        except Exception:
            texte_corrige = texte.strip()
        
        texte_corrige = texte_corrige.lower()
        texte_corrige = re.sub(r'[^\w\s]', ' ', texte_corrige)
        texte_corrige = re.sub(r'\s+', ' ', texte_corrige)
        return texte_corrige.strip()

    def charger_donnees(self) -> None:
        """Charge les donn√©es FAQ depuis le fichier JSON"""
        print("üìÇ Chargement des donn√©es FAQ...")
        try:
            if not os.path.exists(self.data_path):
                raise FileNotFoundError(f"Fichier de donn√©es non trouv√©: {self.data_path}")
            
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            for theme, items in data["faq"].items():
                self.theme_data[theme] = []
                for item in items:
                    question = self.nettoyer_texte(item["question"])
                    reponse = item["response"]
                    self.questions.append(question)
                    self.reponses.append(reponse)
                    self.themes.append(theme)
                    self.theme_data[theme].append({
                        "question": question,
                        "response": reponse
                    })
            
            print(f"‚úÖ {len(self.questions)} questions charg√©es dans {len(self.theme_data)} th√®mes")
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des donn√©es : {e}")
            raise
    
    def entrainer_modeles(self) -> None:
        """Entra√Æne les mod√®les TF-IDF et de classification"""
        print("üéØ Entra√Ænement des mod√®les...")
        if self.questions:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.questions)
            self.classifier.fit(self.questions, self.themes)
            print("‚úÖ Mod√®les entra√Æn√©s avec succ√®s")
        else:
            print("‚ö†Ô∏è Dataset vide, utilisation de l'API par d√©faut")
    
    def detecter_theme(self, question: str) -> str:
        """D√©tecte le th√®me de la question"""
        try:
            question_nettoyee = self.nettoyer_texte(question)
            theme_predit = self.classifier.predict([question_nettoyee])[0]
            return theme_predit
        except Exception:
            return "inconnu"
    
    def trouver_meilleure_reponse(self, question: str) -> Tuple[str, float]:
        """Trouve la meilleure r√©ponse √† une question donn√©e"""
        if not self.questions:
            return "Aucune donn√©e disponible", 0.0
        question_nettoyee = self.nettoyer_texte(question)
        question_vector = self.vectorizer.transform([question_nettoyee])
        similarites = cosine_similarity(question_vector, self.tfidf_matrix).flatten()
        meilleur_index = np.argmax(similarites)
        meilleur_score = similarites[meilleur_index]
        return self.reponses[meilleur_index], meilleur_score
    
    def generer_reponse(self, question: str) -> str:
        """G√©n√®re une r√©ponse √† la question de l'utilisateur avec fallback vers l'API"""
        self.stats["questions_posees"] += 1
        
        if not question.strip():
            return "‚ö†Ô∏è Veuillez poser une question."
        
        question_originale = question
        try:
            question_corrigee = preprocess_question(question)
        except Exception:
            question_corrigee = question.strip()
        
        if question_originale != question_corrigee:
            print(f"üìù Correction: '{question_originale}' ‚Üí '{question_corrigee}'")
        
        theme = self.detecter_theme(question_corrigee)
        reponse, score = self.trouver_meilleure_reponse(question_corrigee)
        
        print(f"üìÅ Th√®me d√©tect√©: {theme.replace('_', ' ').title()}")
        print(f"üéØ Score de confiance: {score:.2f}")
        
        if score >= self.seuil_confiance and self.questions:
            self.stats["reponses_trouvees"] += 1
            return f"‚úÖ {reponse}"
        else:
            # Appel √† l'API OpenRouter avec encodage ASCII-safe
            if not self.client:
                self.stats["questions_incomprises"] += 1
                return "‚ùå Service d'IA indisponible. Veuillez v√©rifier votre configuration."
            
            try:
                # Convertir la question en ASCII-safe
                question_ascii = safe_encode_text(question_corrigee)
                
                # Configuration du mod√®le depuis l'environnement
                model_name = os.getenv("OPENAI_MODEL", "openai/gpt-4o")
                max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "300"))
                temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))
                
                # Message syst√®me configurable
                system_message = os.getenv(
                    "SYSTEM_MESSAGE", 
                    "You are an HR assistant for Nestle. Answer in French in a professional and helpful manner. Limit your response to 200 words maximum."
                )
                
                if os.getenv("DEBUG", "false").lower() == "true":
                    print(f"DEBUG: Question ASCII-safe: {repr(question_ascii)}")
                    print(f"DEBUG: Mod√®le utilis√©: {model_name}")
                
                completion = self.client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {
                            "role": "system",
                            "content": system_message
                        },
                        {
                            "role": "user",
                            "content": question_ascii
                        }
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                if os.getenv("DEBUG", "false").lower() == "true":
                    print("DEBUG: R√©ponse re√ßue de l'API")
                
                ai_answer = completion.choices[0].message.content
                if ai_answer:
                    ai_answer = ai_answer.strip()
                    return f"ü§ñ {ai_answer}"
                else:
                    self.stats["questions_incomprises"] += 1
                    return "‚ùå R√©ponse vide de l'IA. Veuillez reformuler votre question."
                    
            except Exception as e:
                if os.getenv("DEBUG", "false").lower() == "true":
                    print(f"‚ùå Erreur API d√©taill√©e: {str(e)}")
                else:
                    print(f"‚ùå Erreur API: {str(e)}")
                self.stats["questions_incomprises"] += 1
                return "‚ùå Service temporairement indisponible. Veuillez r√©essayer plus tard."
    
    def sauvegarder_modeles(self) -> None:
        """Sauvegarde les mod√®les entra√Æn√©s"""
        model_dir = os.getenv("MODEL_DIR", "model")
        os.makedirs(model_dir, exist_ok=True)
        
        try:
            with open(f"{model_dir}/vectorizer.pkl", 'wb') as f:
                pickle.dump(self.vectorizer, f)
            with open(f"{model_dir}/classifier.pkl", 'wb') as f:
                pickle.dump(self.classifier, f)
            with open(f"{model_dir}/chatbot_data.pkl", 'wb') as f:
                pickle.dump({
                    'questions': self.questions,
                    'reponses': self.reponses,
                    'themes': self.themes,
                    'tfidf_matrix': self.tfidf_matrix
                }, f)
            print("üíæ Mod√®les sauvegard√©s avec succ√®s")
        except Exception as e:
            print(f"‚ùå Erreur lors de la sauvegarde: {e}")
    
    def charger_modeles(self) -> bool:
        """Charge les mod√®les sauvegard√©s"""
        model_dir = os.getenv("MODEL_DIR", "model")
        try:
            with open(f"{model_dir}/vectorizer.pkl", 'rb') as f:
                self.vectorizer = pickle.load(f)
            with open(f"{model_dir}/classifier.pkl", 'rb') as f:
                self.classifier = pickle.load(f)
            with open(f"{model_dir}/chatbot_data.pkl", 'rb') as f:
                data = pickle.load(f)
                self.questions = data['questions']
                self.reponses = data['reponses']
                self.themes = data['themes']
                self.tfidf_matrix = data['tfidf_matrix']
            print("‚úÖ Mod√®les charg√©s depuis le cache")
            return True
        except Exception as e:
            if os.getenv("DEBUG", "false").lower() == "true":
                print(f"‚ö†Ô∏è Impossible de charger les mod√®les depuis le cache : {e}")
            return False
    
    def initialiser(self, force_retrain: bool = False) -> None:
        """Initialise le chatbot"""
        print("üöÄ Initialisation du Chatbot RH Nestl√©...")
        
        # V√©rifier les variables d'environnement critiques
        if not os.getenv("OPENAI_API_KEY"):
            print("‚ö†Ô∏è ATTENTION: Cl√© API OpenAI manquante dans le fichier .env")
        
        if not force_retrain and self.charger_modeles():
            print("‚úÖ Chatbot initialis√© avec les mod√®les sauvegard√©s")
            return
        
        self.charger_donnees()
        self.entrainer_modeles()
        self.sauvegarder_modeles()
        print("‚úÖ Chatbot initialis√© et pr√™t √† r√©pondre!")
    
    def afficher_statistiques(self) -> None:
        """Affiche les statistiques de la session"""
        print("\nüìä STATISTIQUES DE SESSION:")
        print(f"  Questions pos√©es: {self.stats['questions_posees']}")
        print(f"  R√©ponses trouv√©es: {self.stats['reponses_trouvees']}")
        print(f"  Questions incomprises: {self.stats['questions_incomprises']}")
        if self.stats['questions_posees'] > 0:
            taux_reussite = (self.stats['reponses_trouvees'] / self.stats['questions_posees']) * 100
            print(f"  Taux de r√©ussite: {taux_reussite:.1f}%")
    
    def executer(self) -> None:
        """Lance la boucle interactive du chatbot"""
        print("\nüí¨ Chatbot RH Nestl√© - Version S√©curis√©e")
        print("   Tapez 'exit', 'quit' pour quitter\n")
        
        while True:
            try:
                question = input("Vous: ").strip()
                if question.lower() in ['exit', 'quit', 'sortir']:
                    print("üëã Merci d'avoir utilis√© le chatbot RH!")
                    self.afficher_statistiques()
                    break
                if not question:
                    print("‚ö†Ô∏è Veuillez poser une question.")
                    continue
                reponse = self.generer_reponse(question)
                print(f"Bot: {reponse}\n")
            except KeyboardInterrupt:
                print("\nüëã Arr√™t du chatbot.")
                self.afficher_statistiques()
                break
            except Exception as e:
                print(f"‚ùå Erreur: {e}")
                continue

def main():
    """Point d'entr√©e principal"""
    try:
        # V√©rifier si le fichier .env existe
        if not os.path.exists('.env'):
            print("‚ùå Fichier .env manquant. Veuillez le cr√©er avec les variables requises.")
            return
        
        chatbot = ChatbotRHOptimise()
        chatbot.initialiser()
        chatbot.executer()
    except Exception as e:
        print(f"‚ùå Erreur fatale: {e}")

if __name__ == "__main__":
    main()

