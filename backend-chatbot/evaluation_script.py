"""
Script d'√©valuation et d'optimisation du chatbot RH
"""

import json
import numpy as np
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
from chatbot_optimized import ChatbotRHOptimise


class EvaluateurChatbot:
    """
    Classe pour √©valuer et optimiser les performances du chatbot
    """
    
    def __init__(self, chatbot: ChatbotRHOptimise):
        self.chatbot = chatbot
        self.questions_test = []
        self.themes_attendus = []
        self.resultats = {}
    
    def creer_dataset_test(self) -> None:
        """Cr√©e un dataset de test √† partir des donn√©es existantes"""
        print("üîÑ Cr√©ation du dataset de test...")
        
        # Prendre 20% des donn√©es pour le test
        indices = np.random.choice(
            len(self.chatbot.questions), 
            size=max(1, len(self.chatbot.questions) // 5), 
            replace=False
        )
        
        for i in indices:
            self.questions_test.append(self.chatbot.questions[i])
            self.themes_attendus.append(self.chatbot.themes[i])
        
        print(f"‚úÖ {len(self.questions_test)} questions de test cr√©√©es")
    
    def evaluer_classification_themes(self) -> dict:
        """√âvalue la performance de classification des th√®mes"""
        print("üéØ √âvaluation de la classification des th√®mes...")
        
        themes_predits = []
        for question in self.questions_test:
            theme_predit = self.chatbot.detecter_theme(question)
            themes_predits.append(theme_predit)
        
        # Calculer les m√©triques
        accuracy = accuracy_score(self.themes_attendus, themes_predits)
        rapport = classification_report(
            self.themes_attendus, 
            themes_predits, 
            output_dict=True
        )
        
        # Cross-validation
        cv_scores = cross_val_score(
            self.chatbot.classifier, 
            self.chatbot.questions, 
            self.chatbot.themes, 
            cv=5
        )
        
        resultats = {
            'accuracy': accuracy,
            'rapport': rapport,
            'cv_scores': cv_scores,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        print(f"‚úÖ Accuracy: {accuracy:.3f}")
        print(f"‚úÖ CV Score moyen: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})")
        
        return resultats
    
    def evaluer_qualite_reponses(self) -> dict:
        """√âvalue la qualit√© des r√©ponses g√©n√©r√©es"""
        print("üîç √âvaluation de la qualit√© des r√©ponses...")
        
        scores_confiance = []
        reponses_correctes = 0
        
        for question in self.questions_test:
            reponse, score = self.chatbot.trouver_meilleure_reponse(question)
            scores_confiance.append(score)
            
            if score >= self.chatbot.seuil_confiance:
                reponses_correctes += 1
        
        resultats = {
            'score_confiance_moyen': np.mean(scores_confiance),
            'score_confiance_std': np.std(scores_confiance),
            'taux_reponses_correctes': reponses_correctes / len(self.questions_test),
            'scores_confiance': scores_confiance
        }
        
        print(f"‚úÖ Score de confiance moyen: {resultats['score_confiance_moyen']:.3f}")
        print(f"‚úÖ Taux de r√©ponses correctes: {resultats['taux_reponses_correctes']:.3f}")
        
        return resultats
    
    def optimiser_seuil_confiance(self) -> float:
        """Trouve le seuil de confiance optimal"""
        print("‚öôÔ∏è Optimisation du seuil de confiance...")
        
        seuils = np.arange(0.1, 0.9, 0.05)
        meilleurs_resultats = []
        
        for seuil in seuils:
            self.chatbot.seuil_confiance = seuil
            reponses_correctes = 0
            
            for question in self.questions_test:
                reponse, score = self.chatbot.trouver_meilleure_reponse(question)
                if score >= seuil:
                    reponses_correctes += 1
            
            taux_reussite = reponses_correctes / len(self.questions_test)
            meilleurs_resultats.append(taux_reussite)
        
        # Trouver le seuil optimal
        meilleur_index = np.argmax(meilleurs_resultats)
        seuil_optimal = seuils[meilleur_index]
        
        print(f"‚úÖ Seuil optimal trouv√©: {seuil_optimal:.3f}")
        print(f"‚úÖ Taux de r√©ussite avec ce seuil: {meilleurs_resultats[meilleur_index]:.3f}")
        
        return seuil_optimal
    
    def generer_rapport_complet(self) -> None:
        """G√©n√®re un rapport complet d'√©valuation"""
        print("üìä G√©n√©ration du rapport complet...")
        
        # √âvaluations
        resultats_classification = self.evaluer_classification_themes()
        resultats_reponses = self.evaluer_qualite_reponses()
        seuil_optimal = self.optimiser_seuil_confiance()
        
        # Pr√©parer le rapport
        rapport = {
            "evaluation_date": "2024-01-15",
            "dataset_test": {
                "nombre_questions": len(self.questions_test),
                "nombre_themes": len(set(self.themes_attendus))
            },
            "classification_themes": {
                "accuracy": resultats_classification['accuracy'],
                "cv_score_moyen": resultats_classification['cv_mean'],
                "cv_score_std": resultats_classification['cv_std']
            },
            "qualite_reponses": {
                "score_confiance_moyen": resultats_reponses['score_confiance_moyen'],
                "taux_reponses_correctes": resultats_reponses['taux_reponses_correctes']
            },
            "optimisation": {
                "seuil_optimal": seuil_optimal,
                "seuil_actuel": self.chatbot.seuil_confiance
            },
            "recommandations": self.generer_recommandations(resultats_classification, resultats_reponses)
        }
        
        # Sauvegarder le rapport
        with open("evaluation_rapport.json", 'w', encoding='utf-8') as f:
            json.dump(rapport, f, indent=2, ensure_ascii=False)
        
        print("‚úÖ Rapport sauvegard√© dans 'evaluation_rapport.json'")
        self.afficher_resume_rapport(rapport)
    
    def generer_recommandations(self, resultats_classification: dict, resultats_reponses: dict) -> list:
        """G√©n√®re des recommandations d'am√©lioration"""
        recommandations = []
        
        if resultats_classification['accuracy'] < 0.8:
            recommandations.append("Am√©liorer la classification des th√®mes avec plus de donn√©es d'entra√Ænement")
        
        if resultats_reponses['score_confiance_moyen'] < 0.5:
            recommandations.append("Enrichir les donn√©es FAQ avec plus de variantes de questions")
        
        if resultats_reponses['taux_reponses_correctes'] < 0.7:
            recommandations.append("Ajuster le seuil de confiance ou am√©liorer le pr√©processing")
        
        if len(recommandations) == 0:
            recommandations.append("Performance satisfaisante - continuer le monitoring")
        
        return recommandations
    
    def afficher_resume_rapport(self, rapport: dict) -> None:
        """Affiche un r√©sum√© du rapport d'√©valuation"""
        print("\n" + "="*60)
        print("üìã R√âSUM√â DU RAPPORT D'√âVALUATION")
        print("="*60)
        
        print(f"üìä Dataset de test: {rapport['dataset_test']['nombre_questions']} questions")
        print(f"üéØ Accuracy classification: {rapport['classification_themes']['accuracy']:.3f}")
        print(f"üìà Score CV moyen: {rapport['classification_themes']['cv_score_moyen']:.3f}")
        print(f"üîç Score confiance moyen: {rapport['qualite_reponses']['score_confiance_moyen']:.3f}")
        print(f"‚úÖ Taux r√©ponses correctes: {rapport['qualite_reponses']['taux_reponses_correctes']:.3f}")
        print(f"‚öôÔ∏è Seuil optimal: {rapport['optimisation']['seuil_optimal']:.3f}")
        
        print("\nüéØ RECOMMANDATIONS:")
        for i, rec in enumerate(rapport['recommandations'], 1):
            print(f"  {i}. {rec}")
        
        print("="*60)
    
    def executer_evaluation_complete(self) -> None:
        """Ex√©cute une √©valuation compl√®te du chatbot"""
        print("üöÄ D√©marrage de l'√©valuation compl√®te...")
        
        self.creer_dataset_test()
        self.generer_rapport_complet()
        
        print("‚úÖ √âvaluation termin√©e avec succ√®s!")


def main():
    """Point d'entr√©e principal pour l'√©valuation"""
    print("üî¨ √âvaluateur de Performance - Chatbot RH")
    print("="*50)
    
    # Initialiser le chatbot
    chatbot = ChatbotRHOptimise()
    chatbot.initialiser()
    
    # Cr√©er l'√©valuateur
    evaluateur = EvaluateurChatbot(chatbot)
    
    # Ex√©cuter l'√©valuation
    evaluateur.executer_evaluation_complete()


if __name__ == "__main__":
    main()