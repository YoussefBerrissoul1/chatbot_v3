"""
Version ASCII-safe du chatbot pour √©viter les probl√®mes d'encodage
"""

import json
import os
import pickle
import numpy as np
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import unicodedata

# Import conditionnel du correcteur orthographique
try:
    from spellchecker import preprocess_question
except ImportError:
    def preprocess_question(question: str) -> str:
        """Fallback si le correcteur orthographique n'est pas disponible"""
        return question.strip()

def remove_accents(text: str) -> str:
    """Supprime les accents d'un texte pour √©viter les probl√®mes d'encodage"""
    if not text:
        return text
    # Normaliser et supprimer les accents
    nfkd_form = unicodedata.normalize('NFD', text)
    ascii_text = ''.join([c for c in nfkd_form if not unicodedata.combining(c)])
    return ascii_text

def safe_encode_text(text: str) -> str:
    """Encode un texte de mani√®re s√ªre pour l'API"""
    if not text:
        return text
    
    # Remplacer les caract√®res probl√©matiques
    replacements = {
        '√©': 'e', '√®': 'e', '√™': 'e', '√´': 'e',
        '√†': 'a', '√¢': 'a', '√§': 'a',
        '√Æ': 'i', '√Ø': 'i',
        '√¥': 'o', '√∂': 'o',
        '√π': 'u', '√ª': 'u', '√º': 'u',
        '√ß': 'c',
        '√±': 'n'
    }
    
    result = text
    for accent, replacement in replacements.items():
        result = result.replace(accent, replacement)
        result = result.replace(accent.upper(), replacement.upper())
    
    # Supprimer tout caract√®re non-ASCII restant
    result = result.encode('ascii', errors='ignore').decode('ascii')
    return result

class ChatbotRHOptimise:
    """
    Chatbot RH optimis√© avec gestion ASCII-safe
    """
    
    def __init__(self, data_path: str = "data/Nestle-HR-FAQ.json"):
        self.data_path = data_path
        self.seuil_confiance = 1.0
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            stop_words=None,
            lowercase=True,
            min_df=1,
            max_df=0.95
        )
        self.classifier = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=3000, ngram_range=(1, 2))),
            ('nb', MultinomialNB(alpha=0.1))
        ])
        
        # Donn√©es
        self.questions = []
        self.reponses = []
        self.themes = []
        self.tfidf_matrix = None
        self.theme_data = {}
        
        # Statistiques
        self.stats = {
            "questions_posees": 0,
            "reponses_trouvees": 0,
            "questions_incomprises": 0
        }
        
        # Configuration OpenAI
        try:
            self.client = OpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key="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"
            )
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'initialisation du client OpenAI: {e}")
            self.client = None
    
    def nettoyer_texte(self, texte: str) -> str:
        """Nettoie, corrige et normalise le texte"""
        if not texte:
            return ""
        try:
            texte_corrige = preprocess_question(texte)
        except Exception:
            texte_corrige = texte.strip()
        
        texte_corrige = texte_corrige.lower()
        texte_corrige = re.sub(r'[^\w\s]', ' ', texte_corrige)
        texte_corrige = re.sub(r'\s+', ' ', texte_corrige)
        return texte_corrige.strip()

    def charger_donnees(self) -> None:
        """Charge les donn√©es FAQ depuis le fichier JSON"""
        print("üìÇ Chargement des donn√©es FAQ...")
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            for theme, items in data["faq"].items():
                self.theme_data[theme] = []
                for item in items:
                    question = self.nettoyer_texte(item["question"])
                    reponse = item["response"]
                    self.questions.append(question)
                    self.reponses.append(reponse)
                    self.themes.append(theme)
                    self.theme_data[theme].append({
                        "question": question,
                        "response": reponse
                    })
            
            print(f"‚úÖ {len(self.questions)} questions charg√©es dans {len(self.theme_data)} th√®mes")
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des donn√©es : {e}")
            raise
    
    def entrainer_modeles(self) -> None:
        """Entra√Æne les mod√®les TF-IDF et de classification"""
        print("üéØ Entra√Ænement des mod√®les...")
        if self.questions:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.questions)
            self.classifier.fit(self.questions, self.themes)
            print("‚úÖ Mod√®les entra√Æn√©s avec succ√®s")
        else:
            print("‚ö†Ô∏è Dataset vide, utilisation de l'API par d√©faut")
    
    def detecter_theme(self, question: str) -> str:
        """D√©tecte le th√®me de la question"""
        try:
            question_nettoyee = self.nettoyer_texte(question)
            theme_predit = self.classifier.predict([question_nettoyee])[0]
            return theme_predit
        except Exception:
            return "inconnu"
    
    def trouver_meilleure_reponse(self, question: str) -> Tuple[str, float]:
        """Trouve la meilleure r√©ponse √† une question donn√©e"""
        if not self.questions:
            return "Aucune donn√©e disponible", 0.0
        question_nettoyee = self.nettoyer_texte(question)
        question_vector = self.vectorizer.transform([question_nettoyee])
        similarites = cosine_similarity(question_vector, self.tfidf_matrix).flatten()
        meilleur_index = np.argmax(similarites)
        meilleur_score = similarites[meilleur_index]
        return self.reponses[meilleur_index], meilleur_score
    
    def generer_reponse(self, question: str) -> str:
        """G√©n√®re une r√©ponse √† la question de l'utilisateur avec fallback vers l'API"""
        self.stats["questions_posees"] += 1
        
        if not question.strip():
            return "‚ö†Ô∏è Veuillez poser une question."
        
        question_originale = question
        try:
            question_corrigee = preprocess_question(question)
        except Exception:
            question_corrigee = question.strip()
        
        if question_originale != question_corrigee:
            print(f"üìù Correction: '{question_originale}' ‚Üí '{question_corrigee}'")
        
        theme = self.detecter_theme(question_corrigee)
        reponse, score = self.trouver_meilleure_reponse(question_corrigee)
        
        print(f"üìÅ Th√®me d√©tect√©: {theme.replace('_', ' ').title()}")
        print(f"üéØ Score de confiance: {score:.2f}")
        
        if score >= self.seuil_confiance and self.questions:
            self.stats["reponses_trouvees"] += 1
            return f"‚úÖ {reponse}"
        else:
            # Appel √† l'API OpenRouter avec encodage ASCII-safe
            if not self.client:
                self.stats["questions_incomprises"] += 1
                return "‚ùå Service d'IA indisponible. Veuillez reformuler votre question."
            
            try:
                # Convertir la question en ASCII-safe
                question_ascii = safe_encode_text(question_corrigee)
                print(f"DEBUG: Question ASCII-safe: {repr(question_ascii)}")
                
                completion = self.client.chat.completions.create(
                    model="openai/gpt-4o",
                    messages=[
                        {
                            "role": "system",
                            "content": "You are an HR assistant for Nestle. Answer in French in a professional and helpful manner. Limit your response to 200 words maximum."
                        },
                        {
                            "role": "user",
                            "content": question_ascii
                        }
                    ],
                    max_tokens=300,
                    temperature=0.7
                )
                
                print("DEBUG: R√©ponse re√ßue de l'API")
                
                ai_answer = completion.choices[0].message.content
                if ai_answer:
                    ai_answer = ai_answer.strip()
                    return f"ü§ñ {ai_answer}"
                else:
                    self.stats["questions_incomprises"] += 1
                    return "‚ùå R√©ponse vide de l'IA. Veuillez reformuler votre question."
                    
            except Exception as e:
                print(f"‚ùå Erreur API: {str(e)}")
                self.stats["questions_incomprises"] += 1
                return "‚ùå Service temporairement indisponible. Veuillez r√©essayer plus tard."
    
    def sauvegarder_modeles(self) -> None:
        """Sauvegarde les mod√®les entra√Æn√©s"""
        os.makedirs("model", exist_ok=True)
        with open("model/vectorizer.pkl", 'wb') as f:
            pickle.dump(self.vectorizer, f)
        with open("model/classifier.pkl", 'wb') as f:
            pickle.dump(self.classifier, f)
        with open("model/chatbot_data.pkl", 'wb') as f:
            pickle.dump({
                'questions': self.questions,
                'reponses': self.reponses,
                'themes': self.themes,
                'tfidf_matrix': self.tfidf_matrix
            }, f)
        print("üíæ Mod√®les sauvegard√©s avec succ√®s")
    
    def charger_modeles(self) -> bool:
        """Charge les mod√®les sauvegard√©s"""
        try:
            with open("model/vectorizer.pkl", 'rb') as f:
                self.vectorizer = pickle.load(f)
            with open("model/classifier.pkl", 'rb') as f:
                self.classifier = pickle.load(f)
            with open("model/chatbot_data.pkl", 'rb') as f:
                data = pickle.load(f)
                self.questions = data['questions']
                self.reponses = data['reponses']
                self.themes = data['themes']
                self.tfidf_matrix = data['tfidf_matrix']
            print("‚úÖ Mod√®les charg√©s depuis le cache")
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible de charger les mod√®les depuis le cache : {e}")
            return False
    
    def initialiser(self, force_retrain: bool = False) -> None:
        """Initialise le chatbot"""
        print("üöÄ Initialisation du Chatbot RH Nestl√©...")
        if not force_retrain and self.charger_modeles():
            print("‚úÖ Chatbot initialis√© avec les mod√®les sauvegard√©s")
            return
        self.charger_donnees()
        self.entrainer_modeles()
        self.sauvegarder_modeles()
        print("‚úÖ Chatbot initialis√© et pr√™t √† r√©pondre!")
    
    def afficher_statistiques(self) -> None:
        """Affiche les statistiques de la session"""
        print("\nüìä STATISTIQUES DE SESSION:")
        print(f"  Questions pos√©es: {self.stats['questions_posees']}")
        print(f"  R√©ponses trouv√©es: {self.stats['reponses_trouvees']}")
        print(f"  Questions incomprises: {self.stats['questions_incomprises']}")
        if self.stats['questions_posees'] > 0:
            taux_reussite = (self.stats['reponses_trouvees'] / self.stats['questions_posees']) * 100
            print(f"  Taux de r√©ussite: {taux_reussite:.1f}%")
    
    def executer(self) -> None:
        """Lance la boucle interactive du chatbot"""
        print("\nüí¨ Chatbot RH Nestl√© - Version ASCII-Safe")
        print("   Tapez 'exit', 'quit' pour quitter\n")
        while True:
            try:
                question = input("Vous: ").strip()
                if question.lower() in ['exit', 'quit', 'sortir']:
                    print("üëã Merci d'avoir utilis√© le chatbot RH!")
                    self.afficher_statistiques()
                    break
                if not question:
                    print("‚ö†Ô∏è Veuillez poser une question.")
                    continue
                reponse = self.generer_reponse(question)
                print(f"Bot: {reponse}\n")
            except KeyboardInterrupt:
                print("\nüëã Arr√™t du chatbot.")
                self.afficher_statistiques()
                break
            except Exception as e:
                print(f"‚ùå Erreur: {e}")
                continue

def main():
    """Point d'entr√©e principal"""
    try:
        chatbot = ChatbotRHOptimise()
        chatbot.initialiser()
        chatbot.executer()
    except Exception as e:
        print(f"‚ùå Erreur fatale: {e}")

if __name__ == "__main__":
    main()